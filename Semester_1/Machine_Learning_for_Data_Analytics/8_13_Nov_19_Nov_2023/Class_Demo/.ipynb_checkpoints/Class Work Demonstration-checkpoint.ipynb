{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3abcf5f2",
   "metadata": {},
   "source": [
    "# Class Work Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95b90e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading NLTK\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4da069",
   "metadata": {},
   "source": [
    "# Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "104ff92e",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\jose/nltk_data'\n    - 'C:\\\\Users\\\\jose\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\jose\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\jose\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\jose\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sent_tokenize\n\u001b[0;32m      2\u001b[0m text\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mHello Mr. Smith, how are you doing today? The weather is great, and city is awesome. \u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124mThe sky is pinkish-blue. You shouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt eat cardboard\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m----> 4\u001b[0m tokenized_text\u001b[38;5;241m=\u001b[39msent_tokenize(text)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenized_text)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     97\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m load(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlanguage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pickle\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\data.py:750\u001b[0m, in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[1;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m _open(resource_url)\n\u001b[0;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, path \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n\u001b[0;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[0;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\jose/nltk_data'\n    - 'C:\\\\Users\\\\jose\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\jose\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\jose\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\jose\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "text=\"\"\"Hello Mr. Smith, how are you doing today? The weather is great, and city is awesome. \n",
    "The sky is pinkish-blue. You shouldn't eat cardboard\"\"\"\n",
    "tokenized_text=sent_tokenize(text)\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c18736",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Convert 'text' into tokenized form\n",
    "tokenized_word = word_tokenize(text)\n",
    "tokenized_sent = tokenized_word\n",
    "\n",
    "# Display the tokenize text\n",
    "print(tokenized_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b63435",
   "metadata": {},
   "source": [
    "# Frequency Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628a002a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "# Calculate the frequency distribution\n",
    "fdist = FreqDist(tokenized_word)\n",
    "\n",
    "# Display the frequency distribution\n",
    "print(fdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bee906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the most common words\n",
    "fdist.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d42511f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency Distribution Plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fdist.plot(30, cumulative = False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef3743c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Load the stop words\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Display the stop words\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adf25da",
   "metadata": {},
   "source": [
    "# Removing Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06f5686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise an array\n",
    "filtered_sent = []\n",
    "\n",
    "# for loop for the tokenize sentences\n",
    "for w in tokenized_sent:\n",
    "    if w not in stop_words:\n",
    "        filtered_sent.append(w)\n",
    "\n",
    "# Display the tokenize and filtered sentences\n",
    "print(\"Tokenized Sentence:\", tokenized_sent)\n",
    "print(\"Filterd Sentence:\", filtered_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c56ad8",
   "metadata": {},
   "source": [
    "# Steming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629577a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# Create an initialise an object ps by calling a method PorterStemmer()\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# Initialise an array 'stemmed_words'\n",
    "stemmed_words = []\n",
    "\n",
    "# Store all the words into an array 'stemmed_words'\n",
    "for w in filtered_sent:\n",
    "    stemmed_words.append(ps.stem(w))\n",
    "\n",
    "# Display the stemmed_words\n",
    "print(\"Filtered Sentence:\",filtered_sent)\n",
    "print(\"Stemmed Sentence:\",stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74db4839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lexicon Normalization\n",
    "# Performing stemming and Lemmatization\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Create an initialise an object 'lem' by calling a method WordNetLemmatizer()\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "# Create an initialise an object 'stem' by calling a method PorterStemmer()\n",
    "stem = PorterStemmer()\n",
    "\n",
    "# Store the word 'flying' into string 'word'\n",
    "word = \"flying\"\n",
    "\n",
    "# Display the Lemmatized and stemmed words\n",
    "print(\"Lemmatized Word:\", lem.lemmatize(word, \"v\"))\n",
    "print(\"Stemmed Word:\", stem.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d111f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store a sentence into an array 'sent'\n",
    "sent = \"Albert Einstein was born in Ulm, Germany in 1879.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257cce19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading NLTK\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Store the 'sent' into an array 'tokens'\n",
    "tokens = nltk.word_tokenize(sent)\n",
    "\n",
    "# Display tokens\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227bb99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the parts of speech (pos) for the words in the sentence\n",
    "# https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d226c5a",
   "metadata": {},
   "source": [
    "# Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c21bf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# Declare the categories\n",
    "categories = [\n",
    "    'alt.atheism',\n",
    "    'talk.religion.misc',\n",
    "    'comp.graphics',\n",
    "    'sci.space',\n",
    "]\n",
    "\n",
    "# Fetch the 20 news group by calling a method fetch_20newsgroups()\n",
    "groups = fetch_20newsgroups(subset='all', categories=categories)\n",
    "\n",
    "# Create and declare a function is_letter_only()\n",
    "def is_letter_only(word):\n",
    "    for char in word:\n",
    "        if not char.isalpha():\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05add4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import names\n",
    "all_names = set(names.words())\n",
    "all_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1ba137",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Declare and create an empty array\n",
    "data_cleaned = []\n",
    "\n",
    "# Create a loop for all documents\n",
    "for doc in groups.data:\n",
    "    doc = doc.lower()\n",
    "    doc_cleaned = ' '.join(lemmatizer.lemmatize(word) for word in doc.split() if is_letter_only(word) and word not in all_names)\n",
    "    data_cleaned.append(doc_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e80cbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Declare and create an array (count_vector) by calling a method CountVectorizer()\n",
    "count_vector = CountVectorizer(stop_words = \"english\", max_features = None, max_df = 0.5, min_df = 2)\n",
    "data = count_vector.fit_transform(data_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a759362",
   "metadata": {},
   "source": [
    "## Discovering underlying topics in newsgroups\n",
    "A topic model is a type of statistical model for discovering the probability distributions of words linked to the topic. The topic in topic modeling does not exactly match the dictionary definition, but corresponds to a nebulous statistical concept, an abstraction occurs in a collection of documents.\n",
    "\n",
    "When we read a document, we expect certain words appearing in the title or the body of the text to capture the semantic context of the document. An article about Python programming will have words such as class and function, while a story about snakes will have words such as eggs and afraid. Documents usually have multiple topics; for instance, this recipe is about three things, topic modeling, non-negative matrix factorization, and latent Dirichlet allocation, which we will discuss shortly. We can therefore define an additive model for topics by assigning different weights to topics.\n",
    "\n",
    "Topic modeling is widely used for mining hidden semantic structures in given text data. There are two popular topic modeling algorithms—non-negative matrix factorization, and latent Dirichlet allocation. We will go through both of these in the next two sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6330f04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example to understand countvectorizer\n",
    "corpus = [\n",
    "'This is the first document.',\n",
    "'This document is the second document.',\n",
    "'And this is the third one.',\n",
    "'Is this the first document?',\n",
    "]\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Display the feature names in sorted order\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584547dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cc02ab",
   "metadata": {},
   "source": [
    "# Topic modeling using LDA\n",
    "Let's explore another popular topic modeling algorithm, latent Dirichlet allocation (LDA). LDA is a generative probabilistic graphical model that explains each input document by means of a mixture of topics with certain probabilities. Again, topic in topic modeling means a collection of words with a certain connection. In other words, LDA basically deals with two probability values, P(term | topic) and P(topic | document). This can be difficult to understand at the beginning. So, let's start from the bottom, the end result of an LDA model.\n",
    "\n",
    "Let's take a look at the following set of documents:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d64030",
   "metadata": {},
   "source": [
    "* <b>Document 1: This restaurant is famous for fish and chips.</b>\n",
    "* <b>Document 2: I had fish and rice for lunch.</b>\n",
    "* <b>Document 3: My sister bought me a cute kitten.</b>\n",
    "* <b>Document 4: Some research shows eating too much rice is bad.</b>\n",
    "* <b>Document 5: I always forget to feed fish to my cat.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f746a964",
   "metadata": {},
   "source": [
    "* Topic 1: 30% fish, 20% chip, 30% rice, 10% lunch, 10% restaurant (which we can interpret Topic 1 to be food related)\n",
    "* Topic 2: 40% cute, 40% cat, 10% fish, 10% feed (which we can interpret Topic 1 to be about pet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42567ef0",
   "metadata": {},
   "source": [
    "* Documents 1: 85% Topic 1, 15% Topic 2\n",
    "* Documents 2: 88% Topic 1, 12% Topic 2\n",
    "* Documents 3: 100% Topic 2\n",
    "* Documents 4: 100% Topic 1\n",
    "* Documents 5: 33% Topic 1, 67% Topic 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d3bc1a",
   "metadata": {},
   "source": [
    "LDA is trained in a generative manner, where it tries to abstract from the documents a set of hidden topics that are likely to generate a certain collection of words.\n",
    "\n",
    "With all this in mind, let's see LDA in action. The LDA model is also included in scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c5ded9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Declare and initialise a variable t\n",
    "t = 20\n",
    "\n",
    "# Declare and initialise an object 'lda' by calling a method LatentDirichletAllocation()\n",
    "lda = LatentDirichletAllocation(n_components = t, learning_method = 'batch', random_state = 42)\n",
    "\n",
    "# Train the model\n",
    "lda.fit(data)\n",
    "\n",
    "# Print all lda components\n",
    "print(lda.components_)\n",
    "\n",
    "# Get all feature names\n",
    "terms = count_vector.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e34910",
   "metadata": {},
   "source": [
    "Again, we specify 20 topics (n_components). The key parameters of the model are included in the following table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec76a2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename =r'Im1.png', width = 520, height = 350)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ccae23",
   "metadata": {},
   "source": [
    "For the input data to LDA, remember that LDA only takes in term counts as it is a probabilistic graphical model. This is unlike Non-negative Matrix-Factorization (NMF), which can work with both the term count matrix and the tf-idf matrix as long as they are non-negative data. Again, we use the term matrix defined previously as input to the lda model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081224e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "        print(\"Topic {}:\" .format(topic_idx))\n",
    "        print(\" \".join([terms[i] for i in topic.argsort()[-10:]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2f4836",
   "metadata": {},
   "source": [
    "## Reference:\n",
    "* Chapter 8, Python Machine Learning, Sebastian Raschka, Packt Publishing, 2015."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
